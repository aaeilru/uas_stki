{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e66d98b",
   "metadata": {},
   "source": [
    "# Sistem Pencarian Obat Berdasarkan Keluhan dan Nama Obat Berbasis Information Retrieval\n",
    "*Tugas Akhir Mata Kuliah Sistem Temu Kembali Informasi*\n",
    "\n",
    "Notebook ini mencakup seluruh tahapan:\n",
    "- Data Collection (Corpus Generation)  \n",
    "- Preprocessing & Feature Extraction  \n",
    "- Query Processing & Search System  \n",
    "- Evaluation & Testing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de523c",
   "metadata": {},
   "source": [
    "### Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a8252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7664be",
   "metadata": {},
   "source": [
    "### Sastrawi untuk Bahasa Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7e4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sastrawi available\n",
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "    SASTRAWI_AVAILABLE = True\n",
    "    print(\"Sastrawi available\")\n",
    "except ImportError:\n",
    "    SASTRAWI_AVAILABLE = False\n",
    "    print(\"Sastrawi not installed. Install with: pip install Sastrawi\")\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed9297",
   "metadata": {},
   "source": [
    "### Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2744eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 documents\n",
      "\n",
      "Sample Document: OBT001\n",
      "================================================================================\n",
      "DOKUMEN OBAT ID: OBT001\n",
      "================================================================================\n",
      "\n",
      "INFORMASI PRODUK\n",
      "----------------\n",
      "Nama Obat       : Panadol 500mg\n",
      "Nama Generik    : Paracetamol\n",
      "Golongan Obat   : Analgesik Antipiretik\n",
      "Bentuk Sediaan  : Tablet\n",
      "Produsen        : GlaxoSmithKline\n",
      "\n",
      "KOMPOSISI\n",
      "---------\n",
      "Paracetamol 500mg\n",
      "\n",
      "INDIKASI DAN KEGUNAAN\n",
      "---------------------\n",
      "Meredakan demam dan nyeri ringan hingga sedang seperti sakit kepala sakit gigi nyeri haid nyeri otot dan menurunkan\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(corpus_dir='corpus'):\n",
    "    \"\"\"Load all documents from corpus\"\"\"\n",
    "    documents = {}\n",
    "    corpus_path = Path(corpus_dir)\n",
    "    files = sorted(corpus_path.glob('OBT*.txt'))\n",
    "    \n",
    "    for filepath in files:\n",
    "        doc_id = filepath.stem\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        documents[doc_id] = content\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load corpus\n",
    "documents = load_corpus('corpus')\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "# Preview\n",
    "sample_doc_id = list(documents.keys())[0]\n",
    "print(f\"\\nSample Document: {sample_doc_id}\")\n",
    "print(\"=\"*80)\n",
    "print(documents[sample_doc_id][:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0c1e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 metadata records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nama_obat</th>\n",
       "      <th>nama_generik</th>\n",
       "      <th>golongan</th>\n",
       "      <th>komposisi</th>\n",
       "      <th>bentuk_sediaan</th>\n",
       "      <th>indikasi</th>\n",
       "      <th>dosis</th>\n",
       "      <th>efek_samping</th>\n",
       "      <th>kontraindikasi</th>\n",
       "      <th>harga_min</th>\n",
       "      <th>harga_max</th>\n",
       "      <th>perlu_resep</th>\n",
       "      <th>produsen</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBT001</td>\n",
       "      <td>Panadol 500mg</td>\n",
       "      <td>Paracetamol</td>\n",
       "      <td>Analgesik Antipiretik</td>\n",
       "      <td>Paracetamol 500mg</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Meredakan demam dan nyeri ringan hingga sedang...</td>\n",
       "      <td>Dewasa dan anak di atas 12 tahun: 1-2 tablet s...</td>\n",
       "      <td>Mual muntah ruam kulit reaksi alergi kerusakan...</td>\n",
       "      <td>Gangguan fungsi hati berat hipersensitif terha...</td>\n",
       "      <td>5000</td>\n",
       "      <td>15000</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>GlaxoSmithKline</td>\n",
       "      <td>demam;sakit kepala;nyeri;panas;flu;sakit gigi;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OBT002</td>\n",
       "      <td>Biogesic 500mg</td>\n",
       "      <td>Paracetamol</td>\n",
       "      <td>Analgesik Antipiretik</td>\n",
       "      <td>Paracetamol 500mg</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Menurunkan demam dan meredakan nyeri ringan hi...</td>\n",
       "      <td>Dewasa: 1-2 tablet 3-4 kali sehari. Diminum se...</td>\n",
       "      <td>Reaksi alergi kulit mual gangguan fungsi hati ...</td>\n",
       "      <td>Gangguan fungsi hati berat hipersensitif parac...</td>\n",
       "      <td>4000</td>\n",
       "      <td>12000</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Combiphar</td>\n",
       "      <td>demam;panas;sakit kepala;nyeri;flu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OBT003</td>\n",
       "      <td>Bodrex Tablet</td>\n",
       "      <td>Paracetamol + Caffeine</td>\n",
       "      <td>Analgesik</td>\n",
       "      <td>Paracetamol 500mg Caffeine 50mg</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Meringankan sakit kepala dan sakit gigi dengan...</td>\n",
       "      <td>Dewasa: 1-2 tablet 3 kali sehari sesudah makan</td>\n",
       "      <td>Jantung berdebar gelisah insomnia tremor mual</td>\n",
       "      <td>Gangguan jantung tukak lambung aktif insomnia ...</td>\n",
       "      <td>3000</td>\n",
       "      <td>8000</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Tempo Scan Pacific</td>\n",
       "      <td>sakit kepala;migrain;pusing;sakit gigi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OBT004</td>\n",
       "      <td>Paramex Tablet</td>\n",
       "      <td>Paracetamol + Propyphenazone</td>\n",
       "      <td>Analgesik</td>\n",
       "      <td>Paracetamol 250mg Propyphenazone 150mg Caffein...</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Meredakan sakit kepala sakit gigi nyeri otot d...</td>\n",
       "      <td>Dewasa: 1-2 tablet 3 kali sehari setelah makan</td>\n",
       "      <td>Mual muntah pusing gangguan pencernaan reaksi ...</td>\n",
       "      <td>Gangguan fungsi hati dan ginjal berat tukak la...</td>\n",
       "      <td>4000</td>\n",
       "      <td>10000</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Konimex</td>\n",
       "      <td>sakit kepala;sakit gigi;nyeri otot;pusing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OBT005</td>\n",
       "      <td>Oskadon Tablet</td>\n",
       "      <td>Paracetamol + Ibuprofen</td>\n",
       "      <td>Analgesik</td>\n",
       "      <td>Paracetamol 250mg Ibuprofen 200mg Caffeine 50mg</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Meringankan sakit kepala termasuk sakit kepala...</td>\n",
       "      <td>Dewasa: 1 tablet 3 kali sehari sesudah makan</td>\n",
       "      <td>Mual muntah nyeri lambung pusing mengantuk</td>\n",
       "      <td>Tukak lambung atau duodenum gangguan ginjal da...</td>\n",
       "      <td>5000</td>\n",
       "      <td>12000</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>sakit kepala;migrain;sakit gigi;nyeri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       nama_obat                  nama_generik  \\\n",
       "0  OBT001   Panadol 500mg                   Paracetamol   \n",
       "1  OBT002  Biogesic 500mg                   Paracetamol   \n",
       "2  OBT003   Bodrex Tablet        Paracetamol + Caffeine   \n",
       "3  OBT004  Paramex Tablet  Paracetamol + Propyphenazone   \n",
       "4  OBT005  Oskadon Tablet       Paracetamol + Ibuprofen   \n",
       "\n",
       "                golongan                                          komposisi  \\\n",
       "0  Analgesik Antipiretik                                  Paracetamol 500mg   \n",
       "1  Analgesik Antipiretik                                  Paracetamol 500mg   \n",
       "2              Analgesik                    Paracetamol 500mg Caffeine 50mg   \n",
       "3              Analgesik  Paracetamol 250mg Propyphenazone 150mg Caffein...   \n",
       "4              Analgesik    Paracetamol 250mg Ibuprofen 200mg Caffeine 50mg   \n",
       "\n",
       "  bentuk_sediaan                                           indikasi  \\\n",
       "0         Tablet  Meredakan demam dan nyeri ringan hingga sedang...   \n",
       "1         Tablet  Menurunkan demam dan meredakan nyeri ringan hi...   \n",
       "2         Tablet  Meringankan sakit kepala dan sakit gigi dengan...   \n",
       "3         Tablet  Meredakan sakit kepala sakit gigi nyeri otot d...   \n",
       "4         Tablet  Meringankan sakit kepala termasuk sakit kepala...   \n",
       "\n",
       "                                               dosis  \\\n",
       "0  Dewasa dan anak di atas 12 tahun: 1-2 tablet s...   \n",
       "1  Dewasa: 1-2 tablet 3-4 kali sehari. Diminum se...   \n",
       "2     Dewasa: 1-2 tablet 3 kali sehari sesudah makan   \n",
       "3     Dewasa: 1-2 tablet 3 kali sehari setelah makan   \n",
       "4       Dewasa: 1 tablet 3 kali sehari sesudah makan   \n",
       "\n",
       "                                        efek_samping  \\\n",
       "0  Mual muntah ruam kulit reaksi alergi kerusakan...   \n",
       "1  Reaksi alergi kulit mual gangguan fungsi hati ...   \n",
       "2      Jantung berdebar gelisah insomnia tremor mual   \n",
       "3  Mual muntah pusing gangguan pencernaan reaksi ...   \n",
       "4         Mual muntah nyeri lambung pusing mengantuk   \n",
       "\n",
       "                                      kontraindikasi  harga_min  harga_max  \\\n",
       "0  Gangguan fungsi hati berat hipersensitif terha...       5000      15000   \n",
       "1  Gangguan fungsi hati berat hipersensitif parac...       4000      12000   \n",
       "2  Gangguan jantung tukak lambung aktif insomnia ...       3000       8000   \n",
       "3  Gangguan fungsi hati dan ginjal berat tukak la...       4000      10000   \n",
       "4  Tukak lambung atau duodenum gangguan ginjal da...       5000      12000   \n",
       "\n",
       "  perlu_resep            produsen  \\\n",
       "0       Tidak     GlaxoSmithKline   \n",
       "1       Tidak           Combiphar   \n",
       "2       Tidak  Tempo Scan Pacific   \n",
       "3       Tidak             Konimex   \n",
       "4       Tidak               Bayer   \n",
       "\n",
       "                                                tags  \n",
       "0  demam;sakit kepala;nyeri;panas;flu;sakit gigi;...  \n",
       "1                 demam;panas;sakit kepala;nyeri;flu  \n",
       "2             sakit kepala;migrain;pusing;sakit gigi  \n",
       "3          sakit kepala;sakit gigi;nyeri otot;pusing  \n",
       "4              sakit kepala;migrain;sakit gigi;nyeri  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Metadata\n",
    "with open('metadata/obat_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    metadata_list = json.load(f)\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "print(f\"Loaded {len(metadata_df)} metadata records\")\n",
    "\n",
    "# Display sample\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd7537",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38543aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "Total obat: 100\n",
      "\n",
      "Distribusi Kebutuhan Resep:\n",
      "perlu_resep\n",
      "Ya       51\n",
      "Tidak    49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Harga:\n",
      "Min: Rp 2,000\n",
      "Max: Rp 180,000\n",
      "Mean: Rp 13,490 - Rp 31,480\n",
      "\n",
      "Top 10 Golongan Obat:\n",
      "golongan\n",
      "Analgesik                       3\n",
      "Multivitamin                    3\n",
      "Kortikosteroid                  3\n",
      "Antikonvulsan                   3\n",
      "Angiotensin Receptor Blocker    3\n",
      "Analgesik Antipiretik           2\n",
      "Suplemen Herbal                 2\n",
      "Antigout                        2\n",
      "Calcium Channel Blocker         2\n",
      "Benzodiazepin Anxiolytic        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total obat: {len(metadata_df)}\")\n",
    "print(f\"\\nDistribusi Kebutuhan Resep:\")\n",
    "print(metadata_df['perlu_resep'].value_counts())\n",
    "\n",
    "print(f\"\\nHarga:\")\n",
    "print(f\"Min: Rp {metadata_df['harga_min'].min():,}\")\n",
    "print(f\"Max: Rp {metadata_df['harga_max'].max():,}\")\n",
    "print(f\"Mean: Rp {metadata_df['harga_min'].mean():,.0f} - Rp {metadata_df['harga_max'].mean():,.0f}\")\n",
    "\n",
    "# Top golongan\n",
    "print(f\"\\nTop 10 Golongan Obat:\")\n",
    "print(metadata_df['golongan'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6188f",
   "metadata": {},
   "source": [
    "### Build Text Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextPreprocessor initialized\n",
      "Preprocessing documents...\n",
      "Preprocessed 100 documents\n",
      "\n",
      " Sample tokens from OBT001:\n",
      "Total tokens: 93\n",
      "First 30 tokens: ['dokumen', 'obat', 'obt', 'informasi', 'produk', 'nama', 'obat', 'panadol', 'nama', 'generik', 'paracetamol', 'golong', 'obat', 'analgesik', 'antipiretik', 'bentuk', 'sedia', 'produsen', 'glaxosmithkline', 'komposisi', 'paracetamol', 'indikasi', 'guna', 'reda', 'demam', 'nyeri', 'ringan', 'hingga', 'sedang', 'sakit']\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Preprocessing text untuk Bahasa Indonesia\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        if SASTRAWI_AVAILABLE:\n",
    "            self.stemmer = StemmerFactory().create_stemmer()\n",
    "            self.stopword_remover = StopWordRemoverFactory().create_stop_word_remover()\n",
    "            self.custom_stopwords = {\n",
    "                'mg', 'ml', 'tablet', 'kapsul', 'sirup', 'gram', 'yang', 'pada', 'untuk',\n",
    "                'dengan', 'dari', 'atau', 'dan', 'seperti', 'termasuk', 'dapat', 'akan'\n",
    "            }\n",
    "        else:\n",
    "            self.stemmer = None\n",
    "            self.stopword_remover = None\n",
    "            self.custom_stopwords = set()\n",
    "        \n",
    "        print(\"TextPreprocessor initialized\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        if not SASTRAWI_AVAILABLE:\n",
    "            basic_stopwords = {'yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'dengan', 'untuk'}\n",
    "            return [t for t in tokens if t not in basic_stopwords and t not in self.custom_stopwords]\n",
    "        \n",
    "        text = ' '.join(tokens)\n",
    "        text = self.stopword_remover.remove(text)\n",
    "        tokens = text.split()\n",
    "        return [t for t in tokens if t not in self.custom_stopwords and len(t) > 2]\n",
    "    \n",
    "    def stem(self, tokens):\n",
    "        if not SASTRAWI_AVAILABLE or not self.stemmer:\n",
    "            return tokens\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.stem(tokens)\n",
    "        tokens = [t for t in tokens if len(t) > 2]\n",
    "        return tokens\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "\n",
    "# Preprocess All Documents\n",
    "\n",
    "print(\"Preprocessing documents...\")\n",
    "processed_docs = {}\n",
    "\n",
    "for doc_id, content in documents.items():\n",
    "    tokens = preprocessor.preprocess(content)\n",
    "    processed_docs[doc_id] = tokens\n",
    "\n",
    "print(f\"Preprocessed {len(processed_docs)} documents\")\n",
    "\n",
    "# Preview\n",
    "sample_id = list(processed_docs.keys())[0]\n",
    "print(f\"\\n Sample tokens from {sample_id}:\")\n",
    "print(f\"Total tokens: {len(processed_docs[sample_id])}\")\n",
    "print(f\"First 30 tokens: {processed_docs[sample_id][:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84b2840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Statistics:\n",
      "Total tokens: 8305\n",
      "Unique tokens: 776\n",
      "Average tokens per document: 83.0\n",
      "\n",
      "ðŸ” Top 20 most frequent tokens:\n",
      "   obat: 628\n",
      "   nama: 299\n",
      "   guna: 207\n",
      "   informasi: 200\n",
      "   harga: 200\n",
      "   dokumen: 199\n",
      "   cari: 199\n",
      "   generik: 151\n",
      "   hari: 113\n",
      "   gejala: 108\n",
      "   produsen: 107\n",
      "   dosis: 106\n",
      "   butuh: 104\n",
      "   efek: 103\n",
      "   bentuk: 102\n",
      "   kandung: 101\n",
      "   obt: 100\n",
      "   produk: 100\n",
      "   golong: 100\n",
      "   sedia: 100\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "all_tokens = []\n",
    "for tokens in processed_docs.values():\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_freq = Counter(all_tokens)\n",
    "\n",
    "print(f\"Vocabulary Statistics:\")\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "print(f\"Unique tokens: {len(token_freq)}\")\n",
    "print(f\"Average tokens per document: {len(all_tokens) / len(processed_docs):.1f}\")\n",
    "\n",
    "print(f\"\\nðŸ” Top 20 most frequent tokens:\")\n",
    "for token, freq in token_freq.most_common(20):\n",
    "    print(f\"   {token}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5f96f",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b05fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building TF-IDF vectors...\n",
      "   Vocabulary size: 776\n",
      "   Total documents: 100\n",
      "TF-IDF vectors created for 100 documents\n"
     ]
    }
   ],
   "source": [
    "class TFIDFVectorizer:\n",
    "    \"\"\"TF-IDF Vectorizer untuk IR System\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}\n",
    "        self.idf = {}\n",
    "        self.doc_vectors = {}\n",
    "        self.doc_lengths = {}\n",
    "        \n",
    "    def fit_transform(self, documents):\n",
    "        print(\"\\n Building TF-IDF vectors...\")\n",
    "        \n",
    "        # Build vocabulary and calculate document frequency\n",
    "        term_doc_count = defaultdict(int)\n",
    "        all_terms = set()\n",
    "        \n",
    "        for doc_id, tokens in documents.items():\n",
    "            unique_terms = set(tokens)\n",
    "            for term in unique_terms:\n",
    "                term_doc_count[term] += 1\n",
    "            all_terms.update(tokens)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        self.vocabulary = {term: idx for idx, term in enumerate(sorted(all_terms))}\n",
    "        \n",
    "        # Calculate IDF\n",
    "        N = len(documents)\n",
    "        for term, df in term_doc_count.items():\n",
    "            self.idf[term] = math.log10(N / df)\n",
    "        \n",
    "        print(f\"   Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"   Total documents: {N}\")\n",
    "        \n",
    "        # Calculate TF-IDF for each document\n",
    "        for doc_id, tokens in documents.items():\n",
    "            tf = Counter(tokens)\n",
    "            doc_length = len(tokens)\n",
    "            \n",
    "            tfidf_vector = {}\n",
    "            magnitude = 0\n",
    "            \n",
    "            for term, count in tf.items():\n",
    "                if term in self.vocabulary and term in self.idf:\n",
    "                    tf_value = count / doc_length\n",
    "                    tfidf = tf_value * self.idf[term]\n",
    "                    \n",
    "                    term_id = self.vocabulary[term]\n",
    "                    tfidf_vector[term_id] = tfidf\n",
    "                    magnitude += tfidf ** 2\n",
    "            \n",
    "            self.doc_vectors[doc_id] = tfidf_vector\n",
    "            self.doc_lengths[doc_id] = math.sqrt(magnitude)\n",
    "        \n",
    "        print(f\"TF-IDF vectors created for {len(self.doc_vectors)} documents\")\n",
    "        \n",
    "        return self.doc_vectors\n",
    "    \n",
    "    def transform_query(self, query_tokens):\n",
    "        tf = Counter(query_tokens)\n",
    "        query_length = len(query_tokens)\n",
    "        \n",
    "        tfidf_vector = {}\n",
    "        magnitude = 0\n",
    "        \n",
    "        for term, count in tf.items():\n",
    "            if term in self.vocabulary and term in self.idf:\n",
    "                tf_value = count / query_length\n",
    "                tfidf = tf_value * self.idf[term]\n",
    "                \n",
    "                term_id = self.vocabulary[term]\n",
    "                tfidf_vector[term_id] = tfidf\n",
    "                magnitude += tfidf ** 2\n",
    "        \n",
    "        return tfidf_vector, math.sqrt(magnitude)\n",
    "\n",
    "# Build TF-IDF\n",
    "vectorizer = TFIDFVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37da42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Top 10 terms with highest IDF (rare terms):\n",
      "   glaxosmithkline: 2.0000\n",
      "   panadol: 2.0000\n",
      "   rusa: 2.0000\n",
      "   vaksinasi: 2.0000\n",
      "   biogesic: 2.0000\n",
      "   efektivitas: 2.0000\n",
      "   bodrex: 2.0000\n",
      "   propyphenazone: 2.0000\n",
      "   konimex: 2.0000\n",
      "   paramex: 2.0000\n",
      "\n",
      " Top 10 terms with lowest IDF (common terms):\n",
      "   guna: 0.0000\n",
      "   bentuk: 0.0000\n",
      "   ingat: 0.0000\n",
      "   sedia: 0.0000\n",
      "   dokumen: 0.0000\n",
      "   kontraindikasi: 0.0000\n",
      "   pakai: 0.0000\n",
      "   produsen: 0.0000\n",
      "   butuh: 0.0000\n",
      "   kata: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Analyze TF-IDF\n",
    "\n",
    "# Find terms with highest IDF (rare terms)\n",
    "idf_sorted = sorted(vectorizer.idf.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\" Top 10 terms with highest IDF (rare terms):\")\n",
    "for term, idf_value in idf_sorted[:10]:\n",
    "    print(f\"   {term}: {idf_value:.4f}\")\n",
    "\n",
    "print(\"\\n Top 10 terms with lowest IDF (common terms):\")\n",
    "for term, idf_value in idf_sorted[-10:]:\n",
    "    print(f\"   {term}: {idf_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e69f11",
   "metadata": {},
   "source": [
    "### Build Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d760f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"Inverted Index untuk efficient retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(list)\n",
    "        \n",
    "    def build(self, doc_vectors, vocabulary):\n",
    "        print(\"\\nBuilding Inverted Index...\")\n",
    "        \n",
    "        id_to_term = {idx: term for term, idx in vocabulary.items()}\n",
    "        \n",
    "        for doc_id, vector in doc_vectors.items():\n",
    "            for term_id, tfidf in vector.items():\n",
    "                term = id_to_term[term_id]\n",
    "                self.index[term].append((doc_id, tfidf))\n",
    "        \n",
    "        # Sort by tfidf descending\n",
    "        for term in self.index:\n",
    "            self.index[term].sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"Inverted Index built with {len(self.index)} terms\")\n",
    "        \n",
    "        return self.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d9dee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Inverted Index...\n",
      "Inverted Index built with 776 terms\n",
      "\n",
      "Sample Inverted Index:\n",
      "\n",
      "Term: 'dokumen'\n",
      "   Appears in 100 documents\n",
      "   Top 3 documents: [('OBT001', 0.0), ('OBT002', 0.0), ('OBT003', 0.0)]\n",
      "\n",
      "Term: 'obat'\n",
      "   Appears in 100 documents\n",
      "   Top 3 documents: [('OBT001', 0.0), ('OBT002', 0.0), ('OBT003', 0.0)]\n",
      "\n",
      "Term: 'obt'\n",
      "   Appears in 100 documents\n",
      "   Top 3 documents: [('OBT001', 0.0), ('OBT002', 0.0), ('OBT003', 0.0)]\n",
      "\n",
      "Term: 'informasi'\n",
      "   Appears in 100 documents\n",
      "   Top 3 documents: [('OBT001', 0.0), ('OBT002', 0.0), ('OBT003', 0.0)]\n",
      "\n",
      "Term: 'produk'\n",
      "   Appears in 100 documents\n",
      "   Top 3 documents: [('OBT001', 0.0), ('OBT002', 0.0), ('OBT003', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Build inverted index\n",
    "inv_index = InvertedIndex()\n",
    "inverted_index = inv_index.build(doc_vectors, vectorizer.vocabulary)\n",
    "\n",
    "# Sample inverted index\n",
    "sample_terms = list(inverted_index.keys())[:5]\n",
    "print(f\"\\nSample Inverted Index:\")\n",
    "for term in sample_terms:\n",
    "    print(f\"\\nTerm: '{term}'\")\n",
    "    print(f\"   Appears in {len(inverted_index[term])} documents\")\n",
    "    print(f\"   Top 3 documents: {inverted_index[term][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bffdc8",
   "metadata": {},
   "source": [
    "### Query Processing & Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d620162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    \"\"\"Search Engine untuk IR Obat\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor, vectorizer, metadata_dict, doc_vectors, doc_magnitudes):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vectorizer = vectorizer\n",
    "        self.metadata = metadata_dict\n",
    "        \n",
    "        # ini data index dokumen (wajib ada)\n",
    "        self.doc_vectors = doc_vectors          # list/dict: {doc_id: sparse_vec}\n",
    "        self.doc_magnitudes = doc_magnitudes    # list/dict: {doc_id: magnitude}\n",
    "        \n",
    "    def cosine_similarity(self, vec1, vec2, mag1, mag2):\n",
    "        if mag1 == 0 or mag2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        dot_product = sum(\n",
    "            vec1.get(term_id, 0) * vec2.get(term_id, 0)\n",
    "            for term_id in set(vec1.keys()) | set(vec2.keys())\n",
    "        )\n",
    "        return dot_product / (mag1 * mag2)\n",
    "\n",
    "    def search(self, query, top_k=5):\n",
    "        # 1) preprocess query\n",
    "        tokens = self.preprocessor.preprocess(query)\n",
    "\n",
    "        # 2) vectorize query -> harusnya ngembaliin (sparse_vec, magnitude)\n",
    "        q_vec, q_mag = self.vectorizer.transform_query(tokens)\n",
    "\n",
    "        # 3) hitung similarity ke semua dokumen\n",
    "        scored = []\n",
    "        for doc_id, d_vec in self.doc_vectors.items():\n",
    "            d_mag = self.doc_magnitudes.get(doc_id, 0)\n",
    "            score = self.cosine_similarity(q_vec, d_vec, q_mag, d_mag)\n",
    "            if score > 0:\n",
    "                scored.append((doc_id, score))\n",
    "\n",
    "        # 4) ambil top-k\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        top = scored[:top_k]\n",
    "\n",
    "        # 5) bentuk hasil dengan metadata (kalau ada)\n",
    "        results = []\n",
    "        for doc_id, score in top:\n",
    "            meta = self.metadata.get(doc_id, {})\n",
    "            results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"score\": score,\n",
    "                **meta\n",
    "            })\n",
    "\n",
    "        return results, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(self, query, top_k=10):\n",
    "        # Preprocess query\n",
    "        query_tokens = self.preprocessor.preprocess(query)\n",
    "        \n",
    "        if not query_tokens:\n",
    "            return []\n",
    "        \n",
    "        # Transform query to TF-IDF\n",
    "        query_vector, query_magnitude = self.vectorizer.transform_query(query_tokens)\n",
    "        \n",
    "        # Calculate similarity with all documents\n",
    "        scores = []\n",
    "        \n",
    "        for doc_id in self.vectorizer.doc_vectors.keys():\n",
    "            doc_vector = self.vectorizer.doc_vectors.get(doc_id, {})\n",
    "            doc_magnitude = self.vectorizer.doc_lengths.get(doc_id, 0)\n",
    "            \n",
    "            similarity = self.cosine_similarity(\n",
    "                query_vector, doc_vector,\n",
    "                query_magnitude, doc_magnitude\n",
    "            )\n",
    "            \n",
    "            if similarity > 0:\n",
    "                scores.append((doc_id, similarity))\n",
    "        \n",
    "        # Sort by similarity descending\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top-k results with metadata\n",
    "        results = []\n",
    "        for doc_id, score in scores[:top_k]:\n",
    "            metadata = self.metadata.get(doc_id, {})\n",
    "            results.append((doc_id, score, metadata))\n",
    "        \n",
    "        return results, query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc00e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine initialized\n"
     ]
    }
   ],
   "source": [
    "# Build metadata dict\n",
    "metadata_dict = {item['id']: item for item in metadata_list}\n",
    "\n",
    "# Initialize search engine\n",
    "search_engine = SearchEngine(preprocessor, vectorizer, metadata_dict)\n",
    "print(\"Search Engine initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f06774",
   "metadata": {},
   "source": [
    "### Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e74edb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results, query_tokens):\n",
    "    if not results:\n",
    "        print(\"No results found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nQuery tokens: {query_tokens}\")\n",
    "    print(f\"Found {len(results)} results\\n\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    for rank, (doc_id, score, metadata) in enumerate(results, 1):\n",
    "        print(f\"\\n#{rank} | Score: {score:.4f} | ID: {doc_id}\")\n",
    "        print(f\"Nama: {metadata.get('nama_obat', 'N/A')}\")\n",
    "        print(f\"Generik: {metadata.get('nama_generik', 'N/A')}\")\n",
    "        print(f\"Golongan: {metadata.get('golongan', 'N/A')}\")\n",
    "        print(f\"Indikasi: {metadata.get('indikasi', 'N/A')[:150]}...\")\n",
    "        print(f\"Harga: Rp {metadata.get('harga_min', 0):,} - Rp {metadata.get('harga_max', 0):,}\")\n",
    "        print(f\"Resep: {metadata.get('perlu_resep', 'N/A')}\")\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7df921",
   "metadata": {},
   "source": [
    "### Test Case 1: Demam dan Sakit Kepala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a694f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SearchEngine' object has no attribute 'searchengine'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query1 = \u001b[33m\"\u001b[39m\u001b[33mdemam sakit kepala\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results1, tokens1 = \u001b[43msearch_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearchengine\u001b[49m(query1, top_k=\u001b[32m5\u001b[39m)\n\u001b[32m      3\u001b[39m display_results(results1, tokens1)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SearchEngine' object has no attribute 'searchengine'"
     ]
    }
   ],
   "source": [
    "query1 = \"demam sakit kepala\"\n",
    "results1, tokens1 = search_engine.search(query1, top_k=5)\n",
    "display_results(results1, tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d450e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47ddc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
